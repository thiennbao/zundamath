{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-03T14:51:29.398677Z",
     "iopub.status.busy": "2025-01-03T14:51:29.398359Z",
     "iopub.status.idle": "2025-01-03T14:51:29.408389Z",
     "shell.execute_reply": "2025-01-03T14:51:29.407403Z",
     "shell.execute_reply.started": "2025-01-03T14:51:29.398639Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T14:51:29.409720Z",
     "iopub.status.busy": "2025-01-03T14:51:29.409487Z",
     "iopub.status.idle": "2025-01-03T14:53:02.138413Z",
     "shell.execute_reply": "2025-01-03T14:53:02.137332Z",
     "shell.execute_reply.started": "2025-01-03T14:51:29.409697Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install accelerate\n",
    "!pip install bitsandbytes\n",
    "!pip install peft\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install loralib\n",
    "!pip install einops\n",
    "!pip install googletrans==3.1.0a0\n",
    "!pip install huggingface_hub\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T14:53:02.141252Z",
     "iopub.status.busy": "2025-01-03T14:53:02.140929Z",
     "iopub.status.idle": "2025-01-03T14:53:06.047512Z",
     "shell.execute_reply": "2025-01-03T14:53:06.046531Z",
     "shell.execute_reply.started": "2025-01-03T14:53:02.141223Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import transformers\n",
    "from googletrans import Translator\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T12:48:07.987764Z",
     "iopub.status.busy": "2025-01-01T12:48:07.986850Z",
     "iopub.status.idle": "2025-01-01T12:48:07.992329Z",
     "shell.execute_reply": "2025-01-01T12:48:07.991423Z",
     "shell.execute_reply.started": "2025-01-01T12:48:07.987728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"vilm/vinallama-7b-chat\"\n",
    "PROJECT_NAME = \"Group01-ML-Project\"\n",
    "FINE_TUNE_NAME = \"math-vinallama-7b-chat\"\n",
    "USERNAME = \"Namronaldo2004\"\n",
    "HUGGINGFACE_HUB_REPO = USERNAME + \"/\" + PROJECT_NAME\n",
    "FINE_TUNE_REPO = USERNAME + \"/\" + FINE_TUNE_NAME\n",
    "EMAIL = \"thanhdoan0910@gmail.com\"\n",
    "REPO_ACCESS_TOKEN = os.getenv(\"REPO_ACCESS_TOKEN\")\n",
    "FINE_TUNE_ACCESS_TOKEN = os.getenv(\"FINE_TUNE_ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T12:48:10.536702Z",
     "iopub.status.busy": "2025-01-01T12:48:10.536357Z",
     "iopub.status.idle": "2025-01-01T12:48:10.543473Z",
     "shell.execute_reply": "2025-01-01T12:48:10.542816Z",
     "shell.execute_reply.started": "2025-01-01T12:48:10.536671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub.hf_api import HfFolder\n",
    "from huggingface_hub import Repository, HfApi\n",
    "\n",
    "HfFolder.save_token(REPO_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T12:48:14.007833Z",
     "iopub.status.busy": "2025-01-01T12:48:14.006931Z",
     "iopub.status.idle": "2025-01-01T12:56:11.574931Z",
     "shell.execute_reply": "2025-01-01T12:56:11.573899Z",
     "shell.execute_reply.started": "2025-01-01T12:48:14.007790Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "Cloning https://huggingface.co/Namronaldo2004/Group01-ML-Project into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5df0d1344ef4de694f883fbc0c32ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file vinallama-7b-chat/back-up-fine-tune-model/checkpoint-100/optimizer.pt:   0%|          | 6.96k/81…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d46f7e741c44319f407ec715e9d17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file vinallama-7b-chat/back-up-fine-tune-model/runs/Dec27_05-03-14_681e1741ce62/events.out.tfevents.1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb1f7eb69694b45986059b73506a20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file datasets/data-00000-of-00001.arrow:   0%|          | 31.0k/215M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c24860d05b4fd88dd5eb58f3e2a167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file vinallama-7b-chat/back-up-fine-tune-model/runs/Dec27_05-03-14_681e1741ce62/events.out.tfevents.1735…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c2753777ed4b83b46644acd6636dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file vinallama-7b-chat/pretrained_model/adapter_model.safetensors:   0%|          | 32.0k/153M [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d0d0d9d21843b8988cda5d2a02a515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file vinallama-7b-chat/fine-tune-model/checkpoint-100/adapter_model.safetensors:   0%|          | 16.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba09f9a70b747eeb4e7489fa74db8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file vinallama-7b-chat/back-up-fine-tune-model/checkpoint-100/adapter_model.safetensors:   0%|       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dabc615570674cd19b504fd4bb67f8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file math-vinallama-7b-chat/pretrained_model/adapter_model.safetensors:   0%|          | 8.00k/153M […"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92cfdce5cc494b30882a057313b9d326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file vinallama-7b-chat/fine-tune-model/checkpoint-100/optimizer.pt:   0%|          | 16.0k/813M [00:0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfcbfad8a6e949f58ecf1c36acdb2ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file vinallama-7b-chat/fine-tune-model/runs/Dec29_08-29-04_cbc83835ccaf/events.out.tfevents.173546096…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b515acbd235d4792be5dbbcd01d9caad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file vinallama-7b-chat/fine-tune-model/runs/Dec29_08-29-04_cbc83835ccaf/events.out.tfevents.1735460961.c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc622b4fdbb4c11b44fc334bbcb6f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file vinallama-7b-chat/back-up-fine-tune-model/checkpoint-100/rng_state.pth: 100%|##########| 13.9k/1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4641ba9e4450472b9594e58806bbac4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file vinallama-7b-chat/back-up-fine-tune-model/checkpoint-100/rng_state.pth:   7%|7         | 1.00k/13.9…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757bdced9ab943039baf538d9495981a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file vinallama-7b-chat/fine-tune-model/checkpoint-100/rng_state.pth: 100%|##########| 13.9k/13.9k [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762b4c359f774a579589f3a7faa06806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file vinallama-7b-chat/fine-tune-model/checkpoint-100/rng_state.pth:   7%|7         | 1.00k/13.9k [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2c22d8157b499e9ed0a27131a4deeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file vinallama-7b-chat/fine-tune-model/checkpoint-100/training_args.bin: 100%|##########| 5.12k/5.12k…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507534be25564d1d8b4c0c01b20add31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file vinallama-7b-chat/fine-tune-model/checkpoint-100/training_args.bin:  20%|#9        | 1.00k/5.12k [0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9febd0bee54ea29eeede19c2ce51b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file vinallama-7b-chat/back-up-fine-tune-model/checkpoint-100/training_args.bin: 100%|##########| 5.1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d526e0d3373f4c0a8b8de581ad0d3eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file vinallama-7b-chat/back-up-fine-tune-model/checkpoint-100/training_args.bin:  20%|#9        | 1.00k/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51acc764423c491199752ce08bb77a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file vinallama-7b-chat/back-up-fine-tune-model/checkpoint-100/scheduler.pt: 100%|##########| 1.04k/1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d986e1c2674faabe1abe1f2b80a81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file vinallama-7b-chat/back-up-fine-tune-model/checkpoint-100/scheduler.pt:  96%|#########6| 1.00k/1.04k…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7108533f1f4a83a48a740a795eabf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file vinallama-7b-chat/fine-tune-model/checkpoint-100/scheduler.pt:  96%|#########6| 1.00k/1.04k [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22d6b2cd2cd44fdab8ee761784f25ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file vinallama-7b-chat/fine-tune-model/checkpoint-100/adapter_model.safetensors:   0%|          | 1.00k/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25524fc916c244c4ae858002c6f6e011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file vinallama-7b-chat/back-up-fine-tune-model/checkpoint-100/adapter_model.safetensors:   0%|          …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e7275c0e184501b872ca97eccf8bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file vinallama-7b-chat/pretrained_model/adapter_model.safetensors:   0%|          | 1.00k/153M [00:00<?,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7e53e1ca2b40a7b65d41b17bbcd429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file math-vinallama-7b-chat/pretrained_model/adapter_model.safetensors:   0%|          | 1.00k/153M [00:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55773596f3c2474091e5bbcaf3d1694f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file datasets/data-00000-of-00001.arrow:   0%|          | 1.00k/215M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e658907cf54f4ba1a45098f6761e0a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file vinallama-7b-chat/back-up-fine-tune-model/checkpoint-100/optimizer.pt:   0%|          | 1.00k/813M …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42607b9fd69438c914573263709de8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file vinallama-7b-chat/fine-tune-model/checkpoint-100/optimizer.pt:   0%|          | 1.00k/813M [00:00<?…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "repo = Repository(local_dir = PROJECT_NAME,\n",
    "                  git_user = USERNAME,\n",
    "                  git_email = EMAIL,\n",
    "                  clone_from = HUGGINGFACE_HUB_REPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T13:00:54.827221Z",
     "iopub.status.busy": "2025-01-01T13:00:54.826783Z",
     "iopub.status.idle": "2025-01-01T13:00:54.836978Z",
     "shell.execute_reply": "2025-01-01T13:00:54.836064Z",
     "shell.execute_reply.started": "2025-01-01T13:00:54.827189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MicrosoftMathDataset(TorchDataset):\n",
    "    def __init__ (self, start_sample_index = 0, num_get_samples = None, tokenizer = None):\n",
    "        raw_data = None\n",
    "        if (\"datasets\" in os.listdir(PROJECT_NAME)):\n",
    "            directory = f\"{PROJECT_NAME}/datasets\"\n",
    "\n",
    "            arrow_file = [file for file in os.listdir(directory) if file.endswith(\".arrow\")][0]\n",
    "            raw_data = Dataset.from_file(os.path.join(directory, arrow_file))\n",
    "\n",
    "        else:\n",
    "            raw_data = load_dataset(\"microsoft/orca-math-word-problems-200k\")['train']\n",
    "            raw_data.save_to_disk(f\"{PROJECT_NAME}/datasets\")\n",
    "            repo.push_to_hub(commit_message = f\"Push the microsoft/orca-math-word-problems-200k dataset\")\n",
    "\n",
    "        if (num_get_samples is not None):\n",
    "            raw_data = raw_data[start_sample_index: (start_sample_index + num_get_samples)]\n",
    "        else:\n",
    "            raw_data = raw_data[start_sample_index:]\n",
    "\n",
    "        self.question = raw_data['question']\n",
    "        self.answer = raw_data['answer']\n",
    "\n",
    "        self.translator = Translator()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.question)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        question = self.question[index].replace('\\n\\n', ' ').replace('\\n', ' ')\n",
    "        answer = self.answer[index].replace('\\n\\n', ' ').replace('\\n', ' ')\n",
    "\n",
    "        training_sample = self.generate_and_tokenize_prompt(question, answer)\n",
    "        return training_sample\n",
    "\n",
    "    def generate_and_tokenize_prompt(self, question, answer):\n",
    "        def generate_prompt(question, answer):\n",
    "            vn_question = self.translator.translate(question, src = 'en', dest = 'vi').text\n",
    "            vn_answer = self.translator.translate(answer, src = 'en', dest = 'vi').text\n",
    "\n",
    "            return f\"\"\"\n",
    "            <|im_start|>system\n",
    "            Bạn là một chuyên gia về toán. Khi nhận được yêu cầu từ người dùng, dựa vào những kiến thức mà bạn đã có, hãy trả lời người dùng một cách nhất quán, đầy đủ nhưng tránh dư thừa thông tin, và hơn hết nội dung câu trả lời phải chính xác nhất\n",
    "            <|im_end|>\n",
    "            <|im_start|>user\n",
    "            ### Câu hỏi:\n",
    "            {vn_question}\n",
    "            ### Câu trả lời:\n",
    "            <|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            {vn_answer}\n",
    "            \"\"\".strip()\n",
    "\n",
    "        full_prompt = generate_prompt(question, answer)\n",
    "\n",
    "        tokenized_full_prompt = self.tokenizer(\n",
    "            full_prompt,\n",
    "            padding = True,\n",
    "            truncation = True\n",
    "        )\n",
    "\n",
    "        return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T13:00:59.377237Z",
     "iopub.status.busy": "2025-01-01T13:00:59.376282Z",
     "iopub.status.idle": "2025-01-01T13:00:59.385640Z",
     "shell.execute_reply": "2025-01-01T13:00:59.384617Z",
     "shell.execute_reply.started": "2025-01-01T13:00:59.377193Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MyMathDataset(TorchDataset):\n",
    "    def __init__ (self, start_sample_index = 0, num_get_samples = None, tokenizer = None):\n",
    "        raw_data = None\n",
    "        if (\"new_dataset\" in os.listdir(PROJECT_NAME)):\n",
    "            directory = f\"{PROJECT_NAME}/new_dataset\"\n",
    "\n",
    "            arrow_file = [file for file in os.listdir(directory) if file.endswith(\".arrow\")][0]\n",
    "            raw_data = Dataset.from_file(os.path.join(directory, arrow_file))\n",
    "\n",
    "        else:\n",
    "            raw_data = load_dataset(\"Mels22/Vietnamese-Intermediate-Reality-Math-Problems\")['train']\n",
    "            raw_data.save_to_disk(f\"{PROJECT_NAME}/new_dataset\")\n",
    "            repo.push_to_hub(commit_message = f\"Push the Mels22/Vietnamese-Intermediate-Reality-Math-Problems dataset\")\n",
    "\n",
    "        if (num_get_samples is not None):\n",
    "            raw_data = raw_data[start_sample_index: (start_sample_index + num_get_samples)]\n",
    "        else:\n",
    "            raw_data = raw_data[start_sample_index:]\n",
    "\n",
    "        self.question = raw_data['problem']\n",
    "        self.answer = raw_data['solution']\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.question)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        question = self.question[index]\n",
    "        answer = self.answer[index]\n",
    "\n",
    "        training_sample = self.generate_and_tokenize_prompt(question, answer)\n",
    "        return training_sample\n",
    "\n",
    "    def generate_and_tokenize_prompt(self, question, answer):\n",
    "        def generate_prompt(question, answer):\n",
    "            return f\"\"\"\n",
    "            <|im_start|>system\n",
    "            Bạn là một chuyên gia về toán. Khi nhận được yêu cầu từ người dùng, dựa vào những kiến thức mà bạn đã có, hãy trả lời người dùng một cách nhất quán, đầy đủ nhưng tránh dư thừa thông tin, và hơn hết nội dung câu trả lời phải chính xác nhất\n",
    "            <|im_end|>\n",
    "            <|im_start|>user\n",
    "            ### Câu hỏi:\n",
    "            {question}\n",
    "            ### Câu trả lời:\n",
    "            <|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            {answer}\n",
    "            \"\"\".strip()\n",
    "\n",
    "        full_prompt = generate_prompt(question, answer)\n",
    "\n",
    "        tokenized_full_prompt = self.tokenizer(\n",
    "            full_prompt,\n",
    "            padding = True,\n",
    "            truncation = True\n",
    "        )\n",
    "\n",
    "        return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T13:01:01.392531Z",
     "iopub.status.busy": "2025-01-01T13:01:01.392206Z",
     "iopub.status.idle": "2025-01-01T13:01:01.559147Z",
     "shell.execute_reply": "2025-01-01T13:01:01.558024Z",
     "shell.execute_reply.started": "2025-01-01T13:01:01.392502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MathLLM:\n",
    "    def __init__ (self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = None\n",
    "        self.model_name = None\n",
    "        self.tokenizer = None\n",
    "        self.generation_config = None\n",
    "\n",
    "    def get_model (self, model_name, generation_config = None):\n",
    "        # Get folders in the repo\n",
    "        api = HfApi()\n",
    "        files = api.list_repo_files(repo_id = HUGGINGFACE_HUB_REPO)\n",
    "        folders = {file.split('/')[0] for file in files if '/' in file}\n",
    "\n",
    "        if (model_name.split('/')[-1] in folders):\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit = True,\n",
    "                bnb_4bit_use_double_quant = True,\n",
    "                bnb_4bit_quant_type = \"nf4\",\n",
    "                bnb_4bit_compute_dtype = torch.bfloat16\n",
    "            )\n",
    "\n",
    "            if (\"fine-tune-model\" in os.listdir(f\"{PROJECT_NAME}/{model_name.split('/')[-1]}\")):\n",
    "                directory = f\"{PROJECT_NAME}/{model_name.split('/')[-1]}/fine-tune-model\"\n",
    "                checkpoint = [f for f in os.listdir(directory) if f.startswith(\"checkpoint\")][0]\n",
    "                checkpoint_path = os.path.join(directory, checkpoint)\n",
    "                \n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    checkpoint_path,\n",
    "                    device_map = \"auto\",\n",
    "                    trust_remote_code = True,\n",
    "                    quantization_config = bnb_config\n",
    "                )\n",
    "                self.model = PeftModel.from_pretrained(self.model, checkpoint_path)\n",
    "                self.model.gradient_checkpointing_enable()\n",
    "                self.model = prepare_model_for_kbit_training(self.model)\n",
    "            else:\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    f\"{PROJECT_NAME}/{model_name.split('/')[-1]}/pretrained_model\",\n",
    "                    device_map = \"auto\",\n",
    "                    trust_remote_code = True,\n",
    "                    quantization_config = bnb_config\n",
    "                )\n",
    "                self.model = PeftModel.from_pretrained(self.model, f\"{PROJECT_NAME}/{model_name.split('/')[-1]}/pretrained_model\")\n",
    "                self.model.gradient_checkpointing_enable()\n",
    "                self.model = prepare_model_for_kbit_training(self.model)\n",
    "\n",
    "            # Load tokenizer and generation_config from local directories\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(f\"{PROJECT_NAME}/{model_name.split('/')[-1]}/tokenizer\")\n",
    "            self.generation_config = GenerationConfig.from_pretrained(f\"{PROJECT_NAME}/{model_name.split('/')[-1]}/generation_config\")\n",
    "\n",
    "            # Assign the loaded generation_config to the model\n",
    "            self.model.generation_config = self.generation_config\n",
    "            self.model_name = model_name\n",
    "        else:\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit = True,\n",
    "                bnb_4bit_use_double_quant = True,\n",
    "                bnb_4bit_quant_type = \"nf4\",\n",
    "                bnb_4bit_compute_dtype = torch.bfloat16\n",
    "            )\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map = \"auto\",\n",
    "                trust_remote_code = True,\n",
    "                quantization_config = bnb_config\n",
    "            )\n",
    "            self.model_name = model_name\n",
    "\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "            self.model = prepare_model_for_kbit_training(self.model)\n",
    "\n",
    "            config = LoraConfig (\n",
    "                r = 16,\n",
    "                lora_alpha = 32,\n",
    "                target_modules = [\n",
    "                    \"q_proj\",\n",
    "                    \"up_proj\",\n",
    "                    \"o_proj\",\n",
    "                    \"k_proj\",\n",
    "                    \"down_proj\",\n",
    "                    \"gate_proj\",\n",
    "                    \"v_proj\"\n",
    "                ],\n",
    "                lora_dropout = 0.05,\n",
    "                bias = \"none\",\n",
    "                task_type = \"CAUSAL_LM\"\n",
    "            )\n",
    "\n",
    "            self.model = get_peft_model(self.model, config)\n",
    "\n",
    "            if (generation_config is None):\n",
    "                self.generation_config = self.model.generation_config\n",
    "                self.generation_config.temperature = 0.7\n",
    "                self.generation_config.top_p = 0.7\n",
    "                self.generation_config.num_return_sequences = 1\n",
    "                self.generation_config.pad_token_id = self.tokenizer.eos_token_id\n",
    "                self.generation_config.eos_token_id = self.tokenizer.eos_token_id\n",
    "            else:\n",
    "                self.generation_config = generation_config\n",
    "\n",
    "            self.model.save_pretrained(f\"{PROJECT_NAME}/{model_name.split('/')[-1]}/pretrained_model\")\n",
    "            self.tokenizer.save_pretrained(f\"{PROJECT_NAME}/{model_name.split('/')[-1]}/tokenizer\")\n",
    "            self.generation_config.save_pretrained(f\"{PROJECT_NAME}/{model_name.split('/')[-1]}/generation_config\")\n",
    "\n",
    "            repo.push_to_hub(commit_message = \"Upload pretrained model, tokenizer and generation config\")\n",
    "\n",
    "    def fine_tune_LLM (self, train_data, num_epoch = 1, batch_size = 256, learning_rate = 2e-4):\n",
    "        if (self.model is None or self.tokenizer is None):\n",
    "            raise Exception (\"Please provide pretrained model before fine-tuning!\")\n",
    "\n",
    "        self.model.train()\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.dtype in [torch.float32, torch.float16, torch.bfloat16, torch.complex64, torch.complex128]:\n",
    "                param.requires_grad = True\n",
    "\n",
    "        source_dir = f\"{PROJECT_NAME}/{self.model_name.split('/')[-1]}/fine-tune-model\"\n",
    "        dest_dir = f\"{PROJECT_NAME}/{self.model_name.split('/')[-1]}/back-up-fine-tune-model\"\n",
    "\n",
    "        if os.path.exists(source_dir):\n",
    "            # Create a back up directory\n",
    "            if not os.path.exists(dest_dir):\n",
    "                os.mkdir(dest_dir)\n",
    "    \n",
    "            # Delete all files in the back up directory\n",
    "            for item in os.listdir(dest_dir):\n",
    "                item_path = os.path.join(dest_dir, item)\n",
    "                if os.path.isfile(item_path):\n",
    "                    os.remove(item_path)\n",
    "                elif os.path.isdir(item_path):\n",
    "                    shutil.rmtree(item_path)\n",
    "    \n",
    "            # Move previously fine-tuned model to the back up directory\n",
    "            for item_name in os.listdir(source_dir):\n",
    "                source_path = os.path.join(source_dir, item_name)\n",
    "                dest_path = os.path.join(dest_dir, item_name)\n",
    "                \n",
    "                shutil.move(source_path, dest_path)\n",
    "        \n",
    "        training_args = transformers.TrainingArguments(\n",
    "            output_dir = source_dir,\n",
    "            per_device_train_batch_size = 1,\n",
    "            gradient_accumulation_steps = batch_size,\n",
    "            num_train_epochs = num_epoch,\n",
    "            learning_rate = learning_rate,\n",
    "            fp16 = True,\n",
    "            save_total_limit = 1,\n",
    "            logging_steps = 1,\n",
    "            optim = \"paged_adamw_8bit\",\n",
    "            lr_scheduler_type = \"cosine\",\n",
    "            warmup_ratio = 0.05,\n",
    "        )\n",
    "\n",
    "        trainer = transformers.Trainer(\n",
    "            model = self.model,\n",
    "            train_dataset = data,\n",
    "            args = training_args,\n",
    "            data_collator = transformers.DataCollatorForLanguageModeling(self.tokenizer, mlm = False)\n",
    "        )\n",
    "\n",
    "        self.model.config.use_cache = False\n",
    "        trainer.train()\n",
    "        repo.push_to_hub(commit_message = \"Completed training!\")\n",
    "\n",
    "    def generate_answer (self, query):\n",
    "        def get_prompt (system_prompt, query):\n",
    "            return f\"\"\"\n",
    "            <|im_start|>system\n",
    "            {system_prompt}\n",
    "            <|im_end|>\n",
    "            <|im_start|>user\n",
    "            ### Yêu cầu:\n",
    "            {query}\n",
    "            ### Câu trả lời:\n",
    "            <|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            \"\"\".strip()\n",
    "\n",
    "        prompt = get_prompt(\n",
    "            system_prompt = \"Bạn là một chuyên gia về toán. Khi nhận được yêu cầu từ người dùng, dựa vào những kiến thức mà bạn đã có, hãy trả lời người dùng một cách nhất quán, đầy đủ nhưng tránh dư thừa thông tin, và hơn hết nội dung câu trả lời phải chính xác nhất\",\n",
    "            query = query\n",
    "        )\n",
    "\n",
    "        encoding = self.tokenizer(prompt, return_tensors = \"pt\").to(self.device)\n",
    "        with torch.inference_mode():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids = encoding.input_ids,\n",
    "                attention_mask = encoding.attention_mask,\n",
    "                generation_config = self.generation_config\n",
    "            )\n",
    "\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "\n",
    "    def deploy_fine_tune_model (self):\n",
    "        HfFolder.save_token(FINE_TUNE_ACCESS_TOKEN)\n",
    "        fine_tune_repo = Repository(local_dir = FINE_TUNE_NAME,\n",
    "                                    git_user = USERNAME,\n",
    "                                    git_email = EMAIL,\n",
    "                                    clone_from = FINE_TUNE_REPO)\n",
    "    \n",
    "        # Delete all files in the back up directory\n",
    "        for item in os.listdir(FINE_TUNE_NAME):\n",
    "            item_path = os.path.join(FINE_TUNE_NAME, item)\n",
    "            if os.path.isfile(item_path) and item_path != f'{FINE_TUNE_NAME}/.gitattributes':\n",
    "                os.remove(item_path)\n",
    "        \n",
    "        deploy_file_paths = []\n",
    "        tokenizer_dir = f\"{PROJECT_NAME}/{self.model_name.split('/')[-1]}/tokenizer\"\n",
    "        generation_config_dir = f\"{PROJECT_NAME}/{self.model_name.split('/')[-1]}/generation_config\"\n",
    "        fine_tune_dir = f\"{PROJECT_NAME}/{self.model_name.split('/')[-1]}/fine-tune-model\"\n",
    "    \n",
    "        for file in os.listdir(tokenizer_dir):\n",
    "            deploy_file_paths.append(os.path.join(tokenizer_dir, file))\n",
    "    \n",
    "        for file in os.listdir(generation_config_dir):\n",
    "            deploy_file_paths.append(os.path.join(generation_config_dir, file))\n",
    "    \n",
    "        checkpoint = [file for file in os.listdir(fine_tune_dir) if file.startswith(\"checkpoint\")][0]\n",
    "        checkpoint_path = os.path.join(fine_tune_dir, checkpoint)\n",
    "        for file in os.listdir(checkpoint_path):\n",
    "            deploy_file_paths.append(os.path.join(checkpoint_path, file))\n",
    "    \n",
    "        for file_path in deploy_file_paths:\n",
    "            shutil.copy(file_path, FINE_TUNE_NAME)\n",
    "    \n",
    "        fine_tune_repo.push_to_hub(commit_message = \"Update fine-tuned model\")\n",
    "    \n",
    "        HfFolder.save_token(REPO_ACCESS_TOKEN)\n",
    "        shutil.rmtree(FINE_TUNE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T13:01:08.214306Z",
     "iopub.status.busy": "2025-01-01T13:01:08.213302Z",
     "iopub.status.idle": "2025-01-01T15:42:35.217699Z",
     "shell.execute_reply": "2025-01-01T15:42:35.216662Z",
     "shell.execute_reply.started": "2025-01-01T13:01:08.214270Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee47af4d10344e08f279ac991ef55c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/709 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f9e7d6fce042e5bd674ea47ae3d44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a60a33060a43f3ad84bd871492a31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f209e56da04fc782623161d2544c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558ba0ac45d04fff93d010a0a631a74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219f94e37a4646318b368e96dc3758f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/3.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ac222dbc8a4539a718e4e678cf330b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5cf66769b344d1a929d3b3205f6303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74ae825100a482b8ccd42f02de82936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/Namronaldo2004/Group01-ML-Project\n",
      "   8b81cd9..055dd3b  main -> main\n",
      "\n",
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a0aff3386e40e6867729808c4bed8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113923855555185, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250101_130957-nykgg2gv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thanhdoan0910-i-h-c-qu-c-gia-tp-hcm/huggingface/runs/nykgg2gv' target=\"_blank\">Group01-ML-Project/vinallama-7b-chat/fine-tune-model</a></strong> to <a href='https://wandb.ai/thanhdoan0910-i-h-c-qu-c-gia-tp-hcm/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thanhdoan0910-i-h-c-qu-c-gia-tp-hcm/huggingface' target=\"_blank\">https://wandb.ai/thanhdoan0910-i-h-c-qu-c-gia-tp-hcm/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thanhdoan0910-i-h-c-qu-c-gia-tp-hcm/huggingface/runs/nykgg2gv' target=\"_blank\">https://wandb.ai/thanhdoan0910-i-h-c-qu-c-gia-tp-hcm/huggingface/runs/nykgg2gv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 2:23:11, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.922500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.241400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.109000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.761800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.673200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.646000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.614700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.538400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.427200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.419000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.395100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.336200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.321000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.267300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.229000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.237100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.185600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.159400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.125300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.148800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.108400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.125500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.101700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.108900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.116600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.096600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.093700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.098200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.091400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.094800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.095900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa27074a03140a78b4d52f7d0e33c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file vinallama-7b-chat/fine-tune-model/checkpoint-40/optimizer.pt:   0%|          | 1.00/813M [00:00<?,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcb64fff1de49bab751f401b8533737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file vinallama-7b-chat/fine-tune-model/checkpoint-40/scheduler.pt:   0%|          | 1.00/1.04k [00:00<?…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef77d5a483a482ca9a2276437da1b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file vinallama-7b-chat/fine-tune-model/checkpoint-40/training_args.bin:   0%|          | 1.00/5.12k [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e660cdee72a24c8991d09c59b6c4499a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file vinallama-7b-chat/fine-tune-model/checkpoint-40/rng_state.pth:   0%|          | 1.00/13.9k [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89072cd1781b46e79cfa583793d87a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file vinallama-7b-chat/fine-tune-model/runs/Jan01_13-06-57_bd90f49a19f0/events.out.tfevents.1735736832.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd0db76c8314f859a0f2c80cd2108e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file vinallama-7b-chat/fine-tune-model/checkpoint-40/adapter_model.safetensors:   0%|          | 1.00/1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/Namronaldo2004/Group01-ML-Project\n",
      "   055dd3b..597b832  main -> main\n",
      "\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "Cloning https://huggingface.co/Namronaldo2004/math-vinallama-7b-chat into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490020acd7444597b1819230e729e977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file optimizer.pt:   0%|          | 8.00k/813M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89093681e219430682a5625e19f80d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file training_args.bin: 100%|##########| 5.12k/5.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16a13a2dc954d749e3ee6a37c35ee5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file scheduler.pt: 100%|##########| 1.04k/1.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91cafd1ec30346128d6f266279261495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file rng_state.pth: 100%|##########| 13.9k/13.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06100ecac59f4e25bb2a2cf16bdd9988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file training_args.bin:  20%|#9        | 1.00k/5.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55dd24a24574e73b1445288de2dbc00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file adapter_model.safetensors:   0%|          | 24.0k/153M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "825c9588c66348e68b71b8d4c2ac3151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file scheduler.pt:  96%|#########6| 1.00k/1.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0425c03052649bc8a91195b020fc04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file rng_state.pth:   7%|7         | 1.00k/13.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2a805ca75d4116ac001725a5579e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file adapter_model.safetensors:   0%|          | 1.00k/153M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f354072432bc4c6e9e2ad41706d4ea88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file optimizer.pt:   0%|          | 1.00k/813M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b5c2c2976447c4a81ba674d5ac31af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file optimizer.pt:   0%|          | 1.00/813M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6e52b03ae4480a81f295044a3c25af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file adapter_model.safetensors:   0%|          | 1.00/153M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3cdfe4ad044070b7aa10e037efcf37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file training_args.bin:   0%|          | 1.00/5.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee0e3919ddf4b9baf53652c7f8418b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file rng_state.pth:   0%|          | 1.00/13.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42767ab5855406c80249589cdd446f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file scheduler.pt:   0%|          | 1.00/1.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/Namronaldo2004/math-vinallama-7b-chat\n",
      "   68b3bbd..cc14b49  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mGroup01-ML-Project/vinallama-7b-chat/fine-tune-model\u001b[0m at: \u001b[34mhttps://wandb.ai/thanhdoan0910-i-h-c-qu-c-gia-tp-hcm/huggingface/runs/nykgg2gv\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250101_130957-nykgg2gv/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "math_llm = MathLLM()\n",
    "math_llm.get_model(model_name = MODEL_NAME)\n",
    "\n",
    "data = MyMathDataset(start_sample_index = 0, tokenizer = math_llm.tokenizer)\n",
    "math_llm.fine_tune_LLM(train_data = data, num_epoch = 10, batch_size = 100, learning_rate = 2e-4)\n",
    "\n",
    "# response = math_llm.generate_answer(query = \"Hãy cho tôi biết kết quả của phép tính 45 * 6 là bao nhiêu?\")\n",
    "# print(response)\n",
    "\n",
    "math_llm.deploy_fine_tune_model()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
